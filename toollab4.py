# -*- coding: utf-8 -*-
"""toollab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uckGaFIvX6E3Jhd28RmA7LYYisGOGbo5
"""

!pip install wandb -qU
# Log in to your W&B account
import wandb
wandb.login()

wandb.login(relogin=True)
#wandb.init(settings=wandb.Settings(start_method="fork"))
#wandb.init(settings=wandb.Settings(start_method="thread"))

#Rework 18
#change hyperparameter values around, batch size, epochs, and Fdropout

from signal import signal, SIGPIPE, SIG_DFL
signal(SIGPIPE,SIG_DFL)

import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import layers
import wandb
from wandb.keras import WandbMetricsLogger
import matplotlib.pyplot as plt

# get your data
(x_train, y_train), (x_val, y_val) = keras.datasets.cifar100.load_data()

# constant values
NUM_CLASSES = 100 #100 prediction classes
INPUT_SHAPE = (32,32,3) #shape of the input image 32x32 with 3 channels

# hyperparameters you will be tuning
BATCH_SIZE = 100
EPOCHS = 25
LEARNING_RATE = .0001
L1NF = 75
FDROPOUT = .000001

wandb.init(project = 'Tool Lab 3',
          config={
              'bs':BATCH_SIZE,
              'lr':LEARNING_RATE,
              'epochs': EPOCHS,
              'l1nf':L1NF,
              'fdr':FDROPOUT
          },
           name='RUN 0_toollab4: L1NF 16 ' #this is your run name, please number your runs 0-n and tell what you changed
           #change hyperparameter values around, batch size, epochs, and Fdropout
          )

# onehot encode your labels so your model knows its a category
y_train = tf.one_hot(y_train,
                     depth=y_train.max() + 1,
                     dtype=tf.float64)
y_val = tf.one_hot(y_val,
                   depth=y_val.max() + 1,
                   dtype=tf.float64)
  
y_train = tf.squeeze(y_train)
y_val = tf.squeeze(y_val)

# here is a basic model that you will add to
model = tf.keras.models.Sequential([
                  
                  # CHANGE THESE: these are layers you should mix up and change
                  layers.Conv2D(L1NF, (3, 3), input_shape = INPUT_SHAPE,
                                activation='relu',
                                padding='same'),
                  layers.MaxPooling2D(2,2),

                  layers.Conv2D((L1NF * 2), (3, 3), input_shape = INPUT_SHAPE,
                                activation='relu',
                                padding='same'),
                  layers.MaxPooling2D(2, 2),

                  layers.Conv2D(((L1NF * 2)*2), (3, 3), input_shape = INPUT_SHAPE,
                                activation='relu',
                                padding='same'),
                  layers.MaxPooling2D(2, 2),

                  layers.Dropout(FDROPOUT),
                  
                  # DO NOT CHANGE THESE. They should be at the end of your model
                  layers.Flatten(),
                  layers.Dense(NUM_CLASSES, activation='softmax')])


# feel free to experiment with this
model.compile(loss='categorical_crossentropy',
             optimizer=keras.optimizers.RMSprop(learning_rate=LEARNING_RATE),
             # DO NOT CHANGE THE METRIC. This is what you will be judging your model on
             metrics=['accuracy'],)

checkpointCallbackFunction =  tf.keras.callbacks.ModelCheckpoint(
                                filepath='/checkpoints/model.{epoch:02d}-{val_loss:.2f}.h5',
                                save_freq="epoch")

# here you train the model using some of your hyperparameters and send the data
# to weights and biases after every batch            
history = model.fit(x_train, y_train,
                    epochs=EPOCHS,
                    batch_size=BATCH_SIZE,
                    verbose=1,
                    validation_data=(x_val, y_val),
                    callbacks=[WandbMetricsLogger(log_freq='batch'),
                               checkpointCallbackFunction,])

#This will save the model 
model.save("myfile/path/trainedModel.h5")

#This will load my model
model = keras.models.load_model("myfile/path/trainedModel.h5")

print(model)
